{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0721 딥러닝 정규화 및 최적화 실습  Solution (02-01 ~ 02-03)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02-01 Optimizer Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\n",
    "```python\n",
    "#It could be changeable. An integer number between 1 and 60,000 (train data size)\n",
    "batch_size = 1024 \n",
    "```\n",
    "\n",
    "2)\n",
    "```python\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) #Or any optimizer\n",
    "\n",
    "```\n",
    "3)\n",
    "```python\n",
    "optimizers={'sgd': tf.train.GradientDescentOptimizer(learning_rate), 'sgd w/ nesterov': tf.train.MomentumOptimizer(learning_rate, momentum=0.3 ,use_nesterov=True), 'adagrad': tf.train.AdagradOptimizer(learning_rate), 'rmsprop': tf.train.RMSPropOptimizer(learning_rate), 'adam': tf.train.AdamOptimizer(learning_rate)}\n",
    "\n",
    "train_loss_histories = []\n",
    "train_acc_histories = []\n",
    "test_acc_histories = []\n",
    "for opt_name in optimizers:\n",
    "    optimizer = optimizers[opt_name]\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    test_acc_history = []\n",
    "    start_time = time()\n",
    "    for epoch in range(epochs):\n",
    "        for (idx, (images, labels)) in enumerate(train_dataset.take(iterations)):\n",
    "           \n",
    "            # gradient를 계산하고 optimizer를 이용해 back propagation을 수행합니다.\n",
    "            grads = grad(model, images, labels)\n",
    "            \n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "       \n",
    "            # performance를 계산합니다\n",
    "            train_loss = loss_fn(model, images, labels)\n",
    "            train_accuracy = accuracy_fn(model, images, labels)\n",
    "            test_accuracy = accuracy_fn(model, x_test, y_test)\n",
    "            # 그래프를 그리기 위해 기록합니다.\n",
    "            train_loss_history.append(train_loss.numpy())\n",
    "            train_acc_history.append(train_accuracy.numpy())\n",
    "            test_acc_history.append(test_accuracy.numpy())\n",
    "\n",
    "            # 학습 과정을 출력합니다.\n",
    "            if idx % 20 == 0 or idx == iterations - 1:\n",
    "                print(\"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_loss: %.8f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\n",
    "                    % (epoch + 1, idx + 1, iterations, time() - start_time, train_loss, train_accuracy, test_accuracy))\n",
    "    train_loss_histories.append(train_loss_history)\n",
    "    train_acc_histories.append(train_acc_history)\n",
    "    test_acc_histories.append(test_acc_history)\n",
    "plot_fn([key for key in optimizers], train_loss_histories, train_acc_histories, test_acc_histories)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02-01 Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Learning Rate를 10^-1, 10^-2 , ..., 10^-6으로 list를 만들어 실행하십시오\n",
    "\n",
    "```python\n",
    "learning_rates = [10**(-i) for i in range(1,6)]\n",
    "```\n",
    "\n",
    "2) Sceduler Function를 구현하십시오\n",
    "\n",
    "```python\n",
    "# lr: current learning rate\n",
    "#@TODO: learning rate decay 구현\n",
    "##30 epcoh 마다 Learning Rate가 2씩 나눠지도록 구현하십시오\n",
    "def schedule(epoch, lr):\n",
    "    lr: current learning rate\n",
    "    new_lr = lr\n",
    "    if epoch > 0 and epoch % 30 == 0:\n",
    "        new_lr = lr * 0.5\n",
    "    return new_lr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02-03 optimization-Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) # 가장 좋은 validation accuracy를 보인 하이퍼파라미터로 model을 create 하고 training data를 다시 학습합니다. 위에서 보인 가장 좋은 Hyperparameter를 직접 입력해주어도 되고, grid_result.best_params_ 이라는 함수를 사용해도 됩니다.\n",
    "\n",
    "\n",
    "```python\n",
    "best_model = create_model(init=grid_result.best_params_['init'], optimizer=grid_result.best_params_['optimizer'])\n",
    "\n",
    "best_hist = best_model.fit(x_train, y_train, batch_size=grid_result.best_params_['batch_size'], epochs=grid_result.best_params_['nb_epoch'], validation_data=(x_test, y_test))\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
