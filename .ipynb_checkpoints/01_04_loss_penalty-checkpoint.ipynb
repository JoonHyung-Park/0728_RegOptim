{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Loss Panelty\n",
    "\n",
    "이번 실습에서는 Loss Panelty를 이용해 정규화하는 방법에 대해 알아보고자 합니다. 우선 대부분은 Early Stop 실습과 동일하게 진행해줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape([-1, 28 * 28])\n",
    "x_test = x_test.reshape([-1, 28 * 28])\n",
    "\n",
    "m = np.random.randint(0, high=60000, size=1100, dtype=np.int64)\n",
    "x_train = x_train[m]\n",
    "y_train = y_train[m]\n",
    "\n",
    "i = np.arange(1100)\n",
    "np.random.shuffle(i)\n",
    "x_train = x_train[i]\n",
    "y_train = y_train[i]\n",
    "\n",
    "x_valid = x_train[:100]\n",
    "y_valid = y_train[:100]\n",
    "\n",
    "x_train = x_train[100:]\n",
    "y_train = y_train[100:]\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28 * 28])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "n_units = [28 * 28, 512, 512, 10]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "layer = x \n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    layer = tf.matmul(layer, weight) + bias\n",
    "    if i < len(weights) - 1:\n",
    "        layer = tf.nn.tanh(layer)        \n",
    "y_hat = layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Panelty를 이용한 정규화는 loss 함수를 변화시킴으로써 정규화 효과를 얻는 방식을 말합니다. 우리는 이번 실습에서 가장 자주 쓰이는 loss panelty 중 하나인 L2 panelty(weight decay)를 사용해보고자 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hot = tf.one_hot(y, 10)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "regularization_loss = tf.add_n([tf.reduce_sum(tf.square(w)) for w in weights])\n",
    "loss = cross_entropy_loss + 0.001 * regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 다른 부분들은 이전 실습과 동일하게 진행해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.7260 1.9631 1.9846 0.9430 0.8200 0.8446\n",
      "20 1.4420 1.7735 1.7924 0.9950 0.8700 0.8778\n",
      "30 1.3154 1.6809 1.6915 1.0000 0.8700 0.8812\n",
      "40 1.2228 1.5891 1.6040 1.0000 0.8800 0.8843\n",
      "50 1.1426 1.5374 1.5278 1.0000 0.8800 0.8834\n",
      "60 1.0700 1.4791 1.4572 1.0000 0.8700 0.8860\n",
      "70 1.0034 1.4031 1.3919 1.0000 0.8800 0.8862\n",
      "80 0.9417 1.3415 1.3307 1.0000 0.8900 0.8838\n",
      "90 0.8843 1.2805 1.2735 1.0000 0.8900 0.8835\n",
      "100 0.8308 1.2230 1.2198 1.0000 0.8900 0.8833\n",
      "110 0.7807 1.1700 1.1697 1.0000 0.8900 0.8840\n",
      "120 0.7338 1.1286 1.1227 1.0000 0.8900 0.8843\n",
      "130 0.6899 1.0883 1.0783 1.0000 0.8900 0.8839\n",
      "140 0.6488 1.0460 1.0372 1.0000 0.8900 0.8838\n",
      "150 0.6104 1.0138 0.9990 1.0000 0.8800 0.8841\n",
      "160 0.5745 0.9815 0.9628 1.0000 0.8800 0.8826\n",
      "170 0.5410 0.9511 0.9294 1.0000 0.8800 0.8822\n",
      "180 0.5098 0.9176 0.8983 1.0000 0.8700 0.8813\n",
      "0.8838\n"
     ]
    }
   ],
   "source": [
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), tf.argmax(y_hat, 1)),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "weight_values = []\n",
    "for epoch_idx in range(1, 1000 + 1):\n",
    "    session.run(\n",
    "            train_op,\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train\n",
    "            })\n",
    "    \n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        \n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "            weight_values = session.run(weights)\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 100 < epoch_idx:\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "88.05% -> 88.55% 로 성능이 0.5% 향상한 것을 확인할 수 있었습니다.\n",
    "\n",
    "이제 최종 모델 파라미터의 각 element의 distribution을 확인해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = np.concatenate([w.flatten() for w in weight_values], axis=None)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(dist, bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1과 L2를 동시에 사용하는 정규화 panelty term을 디자인할 수도 있을 것입니다. L1이나 L2를 사용할 때, 각 weight matrix 별로 다른 람다값을 사용하면 더 좋은 성능을 가진 네트워크를 가질 수 있지만, 찾아야 하는 파라미터가 너무 많아져 practical하지 않습니다. \n",
    "\n",
    "### 연습 문제\n",
    "\n",
    "Q1. 람다 값을 변경해보면서 test accuracy 와 weight distribution의 변화를 확인해보세요. \n",
    "\n",
    "Q2. L2정규화를 L1으로 바꿔보고, L1에 맞는 람다 값을 찾아보세요. 정규화를 이용해서 성능을 향상시킬 수 있는지, weight distribution에는 어떤 변화가 있는지 확인해봅시다.\n",
    "(힌트는 [여기](01_04_loss_panelty_Q2_hint.txt)에서 확인하실 수 있습니다.)\n",
    "(정답은 [여기](01_04_loss_panelty_Q2_answer.txt)에서 확인하실 수 있습니다.)\n",
    "\n",
    "Q3. (도전과제) L1 정규화를 이용해 각 hidden layer의 적정 hidden unit 갯수를 찾아봅시다. \n",
    "\n",
    "주의사항! 코드를 수정한 이후에는 Kernel > Restart & Run All 을 통해 네트워크를 처음부터 다시 학습시켜 주세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 실습\n",
    "\n",
    "다음 [실습](01_05_hidden_layer_stabilization.ipynb)에서는 Hidden Layer의 statistics을 안정화시키는 방식에 대해서 배워보고자 합니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
