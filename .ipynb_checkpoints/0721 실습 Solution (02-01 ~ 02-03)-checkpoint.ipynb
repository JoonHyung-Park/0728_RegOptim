{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0721 딥러닝 정규화 및 최적화 실습  Solution (02-01 ~ 02-03)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02-01 Optimizer Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\n",
    "```python\n",
    "#It could be changeable. An integer number between 1 and 60,000 (train data size)\n",
    "batch_size = 1024 \n",
    "```\n",
    "\n",
    "2)\n",
    "```python\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) #Or any optimizer\n",
    "\n",
    "```\n",
    "3)\n",
    "```python\n",
    "optimizers={'sgd': tf.train.GradientDescentOptimizer(learning_rate), 'sgd w/ nesterov': tf.train.MomentumOptimizer(learning_rate, momentum=0.3 ,use_nesterov=True), 'adagrad': tf.train.AdagradOptimizer(learning_rate), 'rmsprop': tf.train.RMSPropOptimizer(learning_rate), 'adam': tf.train.AdamOptimizer(learning_rate)}\n",
    "\n",
    "train_loss_histories = []\n",
    "train_acc_histories = []\n",
    "test_acc_histories = []\n",
    "for opt_name in optimizers:\n",
    "    optimizer = optimizers[opt_name]\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    test_acc_history = []\n",
    "    start_time = time()\n",
    "    for epoch in range(epochs):\n",
    "        for (idx, (images, labels)) in enumerate(train_dataset.take(iterations)):\n",
    "           \n",
    "            # gradient를 계산하고 optimizer를 이용해 back propagation을 수행합니다.\n",
    "            grads = grad(model, images, labels)\n",
    "            \n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "       \n",
    "            # performance를 계산합니다\n",
    "            train_loss = loss_fn(model, images, labels)\n",
    "            train_accuracy = accuracy_fn(model, images, labels)\n",
    "            test_accuracy = accuracy_fn(model, x_test, y_test)\n",
    "            # 그래프를 그리기 위해 기록합니다.\n",
    "            train_loss_history.append(train_loss.numpy())\n",
    "            train_acc_history.append(train_accuracy.numpy())\n",
    "            test_acc_history.append(test_accuracy.numpy())\n",
    "\n",
    "            # 학습 과정을 출력합니다.\n",
    "            if idx % 20 == 0 or idx == iterations - 1:\n",
    "                print(\"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_loss: %.8f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\n",
    "                    % (epoch + 1, idx + 1, iterations, time() - start_time, train_loss, train_accuracy, test_accuracy))\n",
    "    train_loss_histories.append(train_loss_history)\n",
    "    train_acc_histories.append(train_acc_history)\n",
    "    test_acc_histories.append(test_acc_history)\n",
    "plot_fn([key for key in optimizers], train_loss_histories, train_acc_histories, test_acc_histories)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02-01 Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Learning Rate를 10^-1, 10^-2 , ..., 10^-6으로 list를 만들어 실행하십시오\n",
    "\n",
    "```python\n",
    "learning_rates = [10**(-i) for i in range(1,6)]\n",
    "```\n",
    "\n",
    "2) Sceduler Function를 구현하십시오\n",
    "\n",
    "```python\n",
    "# lr: current learning rate\n",
    "#@TODO: learning rate decay 구현\n",
    "##30 epcoh 마다 Learning Rate가 2씩 나눠지도록 구현하십시오\n",
    "def schedule(epoch, lr):\n",
    "    lr: current learning rate\n",
    "    new_lr = lr\n",
    "    if epoch > 0 and epoch % 30 == 0:\n",
    "        new_lr = lr * 0.5\n",
    "    return new_lr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01-04 Loss_penalty solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) L2 loss penalty\n",
    "\n",
    "```python\n",
    "regularization_loss = tf.add_n([tf.reduce_sum(tf.square(w)) for w in weights])\n",
    "\n",
    "```\n",
    "\n",
    "2) L1 loss penalty\n",
    "```python\n",
    "regularization_loss = tf.add_n([tf.reduce_sum(tf.math.abs(w)) for w in weights])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01-05 Hidden layer stabilization solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Layer normalization\n",
    "\n",
    "```python\n",
    "layer = tf.contrib.layers.layer_norm(layer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01-06 Dropout solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) training 단계에서만 dropout을 해줘야하는 것이 핵심\n",
    "test 단계에서도 network를 dropout해주면 성능이 하락됩니다.\n",
    "tf.nn.dropout을 단순히 그냥 사용하면 test단게에서도 dropout이 되어 성능하락이 발생\n",
    "\n",
    "Q3) \n",
    "\n",
    "```python\n",
    "layer = tf.cond(training, lambda: tf.nn.dropout(layer, keep_prob=0.5), lambda: layer)\n",
    "```\n",
    "\n",
    "Q4)\n",
    "\n",
    "training과 test과정에서의 scale을 일치시켜주기 위해서 입니다.\n",
    "training단계에서는 몇몇 hidden node들이 비활성화 되어지기 때문에 통상적으로 다음 layer에 넘어가는 activation value들이 적어, 이를 보상해주기 위해  scalining이 필요합니다. \n",
    "\n",
    "Q5)\n",
    "\n",
    "```python \n",
    "layer = tf.keras.layers.GaussianDropout(rate=0.5)(layer, training=training)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01-07 Augmentation solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) Gaussian noise term 앞에 constant를 주어 noise 조절가능 \n",
    "```python\n",
    " tf.cond(training, lambda: tf.random.normal(tf.shape(x), mean=0.0, stddev=50), lambda: tf.zeros_like(x))\n",
    "```\n",
    "Q2) \n",
    "함수에 dataset을 넣어주기전에 dataset reshape 필요 (N, H, W, C)  \n",
    "    (ex. x_train = x_train.reshape(-1,28,28,1))\n",
    "\n",
    "```python\n",
    "\n",
    "def augment_data(dataset, dataset_labels, random_rotation=True, random_shear=True,\n",
    "                 random_shift=True, random_zoom=True):\n",
    "    augmented image = []\n",
    "    augmented_image_labels = []\n",
    "\n",
    "    for num in range (0, dataset.shape[0]):\n",
    "\n",
    "        # Original image\n",
    "        augmented_image.append(dataset[num])\n",
    "        augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "        if random_rotation:\n",
    "            augmented_image.append(tf.contrib.keras.preprocessing.image.random_rotation(dataset[num], 30 ,\n",
    "                row_axis = 0, col_axis = 1, channel_axis = 2))\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "        if random_shear:\n",
    "            augmented_image.append(tf.contrib.keras.preprocessing.image.random_shear(dataset[num], 0.3 ,\n",
    "                row_axis = 0, col_axis = 1, channel_axis = 2))\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "        if random_shift:\n",
    "            augmented_image.append(tf.contrib.keras.preprocessing.image.random_shift(dataset[num], 0.3, 0.3 ,\n",
    "                row_axis = 0, col_axis = 1, channel_axis = 2))\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "        if random_zoom:\n",
    "            augmented_image.append(tf.contrib.keras.preprocessing.image.random_zoom(dataset[num], (0.9,0.9) ,\n",
    "                row_axis = 0, col_axis = 1, channel_axis = 2))\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "    augmented_image = np.array(augmented_image).reshape(-1,28*28)\n",
    "    augmented_image_labels = np.array(augmented_image_labels)\n",
    "\n",
    "    return augmented_image, augmented_image_labels\n",
    "\n",
    "```\n",
    "\n",
    "Q3) \n",
    "\n",
    "```python\n",
    "    ### Q3 answer ###\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    plt.subplot(1,5,1)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(x_train[0])\n",
    "    plt.subplot(1,5,2)\n",
    "    plt.title('Rotated image')\n",
    "    plt.imshow(x_train[1])\n",
    "    plt.subplot(1,5,3)\n",
    "    plt.title('Sheared image')\n",
    "    plt.imshow(x_train[2])\n",
    "    plt.subplot(1,5,4)\n",
    "    plt.title('Shifted image')\n",
    "    plt.imshow(x_train[3])\n",
    "    plt.subplot(1,5,5)\n",
    "    plt.title('Zoomed image')\n",
    "    plt.imshow(x_train[4])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
