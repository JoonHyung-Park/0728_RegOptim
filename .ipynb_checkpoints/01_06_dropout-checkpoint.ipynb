{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Dropout\n",
    "\n",
    "이번 실습에서는 Dropout에 대해서 알아보고자 합니다. Dropout은 대부분의 모델에서 사용할 수 있는 정규화 방법입니다. \n",
    "\n",
    "![image.png](images/04_dropout.png)\n",
    "\n",
    "여러 개의 subnetworks를 앙상블하는 방법으로 해석할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape([-1, 28 * 28]) \n",
    "x_test = x_test.reshape([-1, 28 * 28])\n",
    "\n",
    "m = np.random.randint(0, high=60000, size=1100, dtype=np.int64)\n",
    "x_train = x_train[m]\n",
    "y_train = y_train[m]\n",
    "\n",
    "i = np.arange(1100)\n",
    "np.random.shuffle(i)\n",
    "x_train = x_train[i]\n",
    "y_train = y_train[i]\n",
    "\n",
    "x_valid = x_train[:100]\n",
    "y_valid = y_train[:100]\n",
    "\n",
    "x_train = x_train[100:]\n",
    "y_train = y_train[100:]\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28 * 28])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [28 * 28, 512, 512, 10]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout을 사용하기 위해서는 activation 이후에 dropout layer를 추가하면 됩니다. activation 전에하는 것보다 후에하는 것이 더 좋다고 알려져 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-970e964a7a3b>:6: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f251c30d748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f251c30d748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f251c30d748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f251c30d748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f249fb12be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f249fb12be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f249fb12be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f249fb12be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "layer = x \n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    layer = tf.matmul(layer, weight) + bias\n",
    "    if i < len(weights) - 1:\n",
    "        layer = tf.nn.tanh(layer)  \n",
    "        layer = tf.layers.dropout(layer, rate=0.5, training=training)\n",
    "y_hat = layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 다른 부분들은 이전 실습과 동일하게 진행해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.4959 0.5215 0.6271 0.8480 0.8700 0.8015\n",
      "20 0.2940 0.4703 0.4901 0.9100 0.8600 0.8474\n",
      "30 0.1858 0.4601 0.4454 0.9370 0.8600 0.8625\n",
      "40 0.1135 0.4648 0.4150 0.9640 0.8800 0.8762\n",
      "50 0.0692 0.3962 0.4010 0.9820 0.8700 0.8808\n",
      "60 0.0494 0.4206 0.4038 0.9870 0.8600 0.8838\n",
      "70 0.0315 0.4087 0.3954 0.9940 0.8800 0.8855\n",
      "80 0.0229 0.4535 0.3992 0.9960 0.8800 0.8878\n",
      "90 0.0169 0.4431 0.3969 0.9980 0.8900 0.8905\n",
      "100 0.0133 0.4294 0.4091 0.9980 0.8600 0.8905\n",
      "110 0.0113 0.4569 0.4058 0.9980 0.8600 0.8909\n",
      "120 0.0099 0.4932 0.4147 0.9980 0.8600 0.8917\n",
      "130 0.0069 0.5003 0.4130 0.9990 0.8500 0.8960\n",
      "140 0.0055 0.4887 0.4167 1.0000 0.8900 0.8947\n",
      "150 0.0051 0.4705 0.4059 0.9990 0.8800 0.8964\n",
      "160 0.0043 0.4480 0.4071 1.0000 0.9000 0.8987\n",
      "170 0.0043 0.4935 0.4172 1.0000 0.8700 0.8996\n",
      "180 0.0033 0.4971 0.4160 1.0000 0.8600 0.8959\n",
      "190 0.0043 0.4558 0.4227 0.9990 0.8900 0.8992\n",
      "200 0.0029 0.4594 0.4273 1.0000 0.8900 0.8985\n",
      "210 0.0025 0.4558 0.4244 1.0000 0.9000 0.9018\n",
      "220 0.0019 0.4132 0.4319 1.0000 0.9000 0.8993\n",
      "230 0.0018 0.4096 0.4301 1.0000 0.8900 0.9002\n",
      "240 0.0019 0.4940 0.4374 1.0000 0.8800 0.8971\n",
      "250 0.0017 0.4815 0.4404 1.0000 0.8800 0.8994\n",
      "260 0.0016 0.4516 0.4511 1.0000 0.8800 0.8980\n",
      "0.8987\n",
      "0.017659607518221685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_hot = tf.one_hot(y, 10)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "loss = cross_entropy_loss\n",
    "\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), tf.argmax(y_hat, 1)),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "\n",
    "import time \n",
    "times = []\n",
    "for epoch_idx in range(1, 1000 + 1):\n",
    "    start_time = time.time()\n",
    "    session.run(\n",
    "            train_op,\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train,\n",
    "                training: True\n",
    "            })\n",
    "    times.append(time.time() - start_time)\n",
    "    \n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        \n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 100 < epoch_idx:\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)\n",
    "print(sum(times) / len(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "87.10% -> 89.87% 성능이 향상됨을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제\n",
    "\n",
    "Q1. Dropout Rate을 바꿔가면서 성능을 확인해봅시다. \n",
    "\n",
    "Q2. tf.nn.dropout를 이용해서 dropout을 다시 구현해봅시다. tf.layers.dropout을 사용할 때와 비슷한 성능이 나오나요?\n",
    "(참고: [API 문서](https://www.tensorflow.org/api_docs/python/tf/nn/dropout))\n",
    "([여기](01_06_dropout_Q2_answer.txt)에서 정답을 확인할 수 있습니다.)\n",
    "\n",
    "Q3. [tf.layers.dropout API 문서](https://www.tensorflow.org/api_docs/python/tf/layers/dropout)에 보면 다음과 같은 서술이 있습니다.\n",
    "```\n",
    "The units that are kept are scaled by 1 / (1 - rate)\n",
    "```\n",
    "이 구현이 왜 필요한지 생각해봅시다.\n",
    "\n",
    "Q4. tf.keras.layers.GaussianDropout를 이용해 Gaussian Dropout을 구현해봅시다. 더 빠른 수렴이 가능해 지나요? Gaussian Dropout에 대해서도 dropout rate을 변경해보면서 성능 그래프를 그려봅시다. \n",
    "(정답은 [여기](01_06_dropout_Q4_answer.txt)에서 확인할 수 있습니다.)\n",
    "\n",
    "Q5. (도전과제) DropConnect를 구현해보고, Dropout보다 성능이 실제 증가하는지 확인해봅시다. \n",
    "\n",
    "주의사항! 코드를 수정한 이후에는 Kernel > Restart & Run All 을 통해 네트워크를 처음부터 다시 학습시켜 주세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 실습 \n",
    "\n",
    "이제 다음 [실습](01_07_augmentation.ipynb)에서는 Augmentation을 적용하는 방법에 대해 알아보도록 하겠습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
